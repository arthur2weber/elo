# ELO Automation Engine

ELO is a local automation engine that **observes events**, **asks Gemini for logic**, and **executes TypeScript automations**. It generates clean `.ts` scripts in `automations/` and runs them directly.

## ðŸŽ¯ Vision

ELO is meant to be a **proactive smartâ€‘home butler** that constantly adapts automations based on user preferences and daily device logs. The intended behavior is:

- Use AI decisions to **generate and refactor TypeScript automations**.
- Treat automations as living flows: **nothing is fixed**, and logic evolves as the userâ€™s tastes change.
- Learn from **continuous device logs** to keep the house optimized and responsive.

> Note: The current codebase does not implement this full vision yet; the section above describes the intended direction of the project.

## âœ… What it really does today

- **Creates automation scripts** in `automations/` (TypeScript).
- **Lists and updates automations** from the CLI.
- **Stores smartâ€‘home device logs** in `logs/events.jsonl` via the CLI.
- **Monitors registered devices** and logs only state changes.
- **Stores user requests** in `logs/requests.jsonl`.
- **Runs a decision loop** that updates automations from logs + requests + preferences.
- **Provides structured device context** (registry + status snapshot) to Gemini.
- **Learns preference patterns** from user decisions (accept/reject suggestions).
- **Optional network discovery** (mDNS) to detect new devices.

## ðŸš« What it does NOT do yet

- It **does not learn preferences automatically**; preferences come from recorded decisions and are summarized on demand.
- Preference learning today is **rule-based** (acceptance rate thresholds), not a full ML model.
- There is no sandboxing/permission system for automations yet.

## Project structure (real files in use)

- `src/cli/` â€” CLI entry and commands.
- `src/ai/` â€” Gemini API client and prompt generator.
- `src/server/` â€” automation engine + device monitor + decision loop.
- `automations/` â€” TypeScript scripts generated by the CLI or AI.
- `logs/` â€” device logs stored as JSON Lines (created by the CLI).
- `logs/decisions.jsonl` â€” user acceptance/rejection history.
- `logs/requests.jsonl` â€” user requests history.
- `logs/devices.json` â€” device registry used by the monitor.

> Note: `logs/`, `automations/`, and `backups/` are local, user-specific, and not tracked in git.

## Setup

Install dependencies:

```bash
npm install
```

## Dockerized Engine (Gemini API only)

The `elo-core` service runs the engine inside Docker and uses the **Gemini API** only.
It builds from `docker/elo/Dockerfile`. In production mode it compiles TypeScript (`npm run build`) and executes optimized JavaScript, reducing RAM usage.

The container uses these runtime environment variables (same as local usage):

- `GEMINI_API_KEY` (required to enable AI)
- `GEMINI_API_MODEL` (default: `gemini-1.5-flash`)
- `GEMINI_API_BASE_URL` (default: `https://generativelanguage.googleapis.com/v1beta`)
- `THINKING_BUDGET` (optional override; set to -1 for auto sizing)

## Gemini API (Google AI Studio)

Authentication is **API-key based only**. Create a Gemini API key in Google AI Studio and export it as `GEMINI_API_KEY`.
When the key is not set, AI features are disabled.

Environment variables used by the code:

- `GEMINI_API_KEY` (required to enable AI)
- `GEMINI_API_MODEL` (default: `gemini-1.5-flash`)
- `GEMINI_API_BASE_URL` (default: `https://generativelanguage.googleapis.com/v1beta`)
- `THINKING_BUDGET` (optional override; set to -1 for auto sizing)
- `ELO_FILES_PATH` (default: project root)
- `ELO_MONITOR_ENABLED` (default: true)
- `ELO_MONITOR_INTERVAL_MS` (default: 5000)
- `ELO_HEALTH_URL` (optional: endpoint to ping for health logging)
- `ELO_DISCOVERY_ENABLED` (default: true)
- `ELO_DISCOVERY_ACTIVE_SCAN` (default: true)
- `ELO_DISCOVERY_SUBNET` (optional: override subnet base, e.g. `192.168.1.0/24`)
- `ELO_DISCOVERY_RANGE` (optional: explicit range, e.g. `192.168.1.10-192.168.1.200`)
- `ELO_DISCOVERY_PORTS` (default: `4387,554,8899`)
- `ELO_DISCOVERY_SCAN_TIMEOUT_MS` (default: `250`)
- `ELO_DISCOVERY_SCAN_CONCURRENCY` (default: `64`)
- `ELO_DISCOVERY_SCAN_INTERVAL_MS` (default: `0` = only on startup)
- `ELO_SSDP_INTERVAL_MS` (default: `60000`)
- `ELO_DISCOVERY_PLUGINS_FILE` (optional: JSON config file for vendor discovery plugins; see below)
- `ELO_FINGERPRINT_AI` (default: true when GEMINI_API_KEY is set)
- `ELO_FINGERPRINT_MODEL` (default: `gemini-2.5-flash`)
- `ELO_FINGERPRINT_TIMEOUT_MS` (default: `1500`)
- `ELO_WOL_ENABLED` (default: true)
- `ELO_WOL_BROADCAST` (default: `255.255.255.255`)
- `ELO_DECISION_LOOP_ENABLED` (default: true)
- `ELO_DECISION_INTERVAL_MS` (default: 10000)
- `ELO_DECISION_LOG_LIMIT` (default: 100)
- `ELO_DECISION_REQUEST_LIMIT` (default: 50)
- `ELO_DECISION_AUTOMATIONS` (comma-separated automation names)
- `ELO_AI_APPROVAL` (set to true to let Gemini decide approval thresholds)
- `ELO_AI_REPLY` (set to true to let Gemini parse ambiguous user replies)

Automation creation/refactor requests auto-scale the thinking budget between 4000â€“16000 based on context size.
Simple Q&A prompts use a thinking budget of 0.

Network discovery uses mDNS (including `_services._dns-sd._udp`, AirPlay, RAOP, printer) when the optional `bonjour-service` dependency is installed, plus an active TCP scan, SSDP listener, and a UDP broadcast for Gree devices. SSDP responses with Samsung/Tizen headers are tagged automatically. If `bonjour-service` is missing, ELO still performs the active scan/broadcast and logs the results.

### Discovery plugin config (optional)

If you have devices from other brands, you can move vendor-specific discovery settings out of env vars into a small JSON file.
Set `ELO_DISCOVERY_PLUGINS_FILE` (or use `config/discovery.plugins.json` in the project root).
Environment variables still override the file when set.

Legacy env vars still supported (prefer the plugin file for these):

- `ELO_GREE_BROADCAST_ENABLED` (default: true)
- `ELO_GREE_BROADCAST_PORTS` (default: `4387`)
- `ELO_GREE_BROADCAST_INTERVAL_MS` (default: `60000`)
- `ELO_GREE_BROADCAST_PAYLOAD` (default: `{"t":"scan"}`)
- `ELO_SSDP_ENABLED` (default: true)

Example `config/discovery.plugins.json`:

```json
{
	"greeBroadcast": {
		"enabled": true,
		"ports": [4387],
		"intervalMs": 60000,
		"payload": {"t": "scan"}
	},
	"ssdp": {
		"enabled": true,
		"intervalMs": 60000
	}
}
```

### Optional: Nmap discovery script

If you want a deeper scan (similar to Home Assistant), you can run the bundled Nmap helper. It discovers live hosts and then scans selected TCP/UDP ports (including Samsung 8001/8002/1515), ingesting results into `logs/events.jsonl`. If a Samsung MAC is detected but those ports are closed, it will send a Wake-on-LAN packet.

The ingestion step also captures extra metadata (MAC/vendor, open ports, HTTP headers + title snippet, RTSP server banner, TCP banners, UDP probe responses, SSDP LOCATION parsing) to help the AI identify devices and protocols.

Environment knobs used by the script:

- `ELO_DISCOVERY_SUBNET` (optional, e.g. `192.168.16.0/24`)
- `ELO_NMAP_PORTS` (default: `80,443,8080,554,8899,8000,22`)
- `ELO_NMAP_UDP_PORTS` (default: `4387,1900`)

Run it from the project root:

```bash
chmod +x scripts/nmap-discovery.sh
sudo ./scripts/nmap-discovery.sh
```

## CLI usage (tested behavior)

```bash
npm run cli create-automation "Office Comfort" --ai --description "Keep office at 23C when occupied"
npm run cli list-automations
npm run cli update-automation "Office Comfort" --ai --log-limit 100
npm run cli add-log --device "thermostat" --event "temperature" --payload '{"value":23}'
npm run cli add-device --id "office-thermostat" --name "Thermostat" --room "office" --endpoint "http://localhost:8081/status"
npm run cli add-request --request "Ajuste o ar para 23C" --user "arthur" --context "office"
npm run cli record-decision --action-key "set-office-temp-23" --suggestion "Adjust office temp" --accepted
npm run cli summarize-preferences
npm run cli list-suggestions
npm run cli approve-suggestion <id>
npm run cli reject-suggestion <id>
npm run cli reply-suggestion <id> "Aham, mas coloca no 21c"
```

## Suggestion flow (proactivity)

When the decision loop proposes a change, it creates a **suggestion** entry (`logs/suggestions.jsonl`).

- If `ELO_AI_APPROVAL=true` or `GEMINI_API_KEY` is set, Gemini decides whether to auto-apply or request more approvals.
- Otherwise, a simple fallback rule is used (>=3 approvals, >=70% acceptance).

## Natural replies and lexicon

User replies are interpreted locally first, using `logs/lexicon.json` (optional). If the reply is long or ambiguous, ELO consults Gemini to extract intent and extra instructions.
When Gemini is used, it also returns the **matched term** (e.g., "claro", "nÃ£o"), which is stored back into `logs/lexicon.json` to adapt to the user's language.

## Example flow (smart-home butler)

1) **Ingest device logs** during the day:

```bash
npm run cli add-log --device "office-thermostat" --event "temperature" --payload '{"value":27}'
npm run cli add-log --device "office-thermostat" --event "temperature" --payload '{"value":28}'
```

2) **Record user decisions** when the butler suggests something:

```bash
npm run cli record-decision --action-key "set-office-temp-23" \
	--suggestion "Adjust office temperature to 23C and enable silent mode" \
	--accepted
```

3) **Review inferred preferences**:

```bash
npm run cli summarize-preferences
```

4) **Update the automation using AI + logs + preferences**:

```bash
npm run cli update-automation "Office Comfort" --ai --log-limit 100
```

## AI decision examples (based on device status + requests)

Example 1:

- User request: "Ligue o ar condicionado do escritÃ³rio"
- Device status: window is open
- Expected AI behavior: ask for confirmation before turning on AC.

Example 2:

- Calendar: meeting at 19h
- Device status: office at 29Â°C, window open
- Expected AI behavior: skip turning on AC until the window is closed, log the reason, and notify later.

## Docker (optional)

`docker-compose.yml` runs the ELO engine container with host networking for device discovery.
For active scans (Nmap/UDP), the container runs as root with `NET_RAW`/`NET_ADMIN` capabilities enabled.

```bash
docker-compose up --build
```

## Always-on monitoring

When the server starts, it polls registered devices (default 5s interval) and appends status logs only when state changes.

```bash
npm start
```

Environment variables:

- `ELO_MONITOR_ENABLED` (default: true)
- `ELO_MONITOR_INTERVAL_MS` (default: 5000) - Polling interval (5s) to save I/O
- `ELO_HEALTH_URL` (optional: health endpoint to ping)
- `ELO_DECISION_LOOP_ENABLED` (default: true)
- `ELO_DECISION_INTERVAL_MS` (default: 10000)
- `ELO_DECISION_LOG_LIMIT` (default: 100)
- `ELO_DECISION_REQUEST_LIMIT` (default: 50)
- `ELO_DECISION_AUTOMATIONS` (comma-separated automation names)

## Smoke test

This repo includes a small smoke test that validates file-based automation creation.

```bash
npm run smoke
```