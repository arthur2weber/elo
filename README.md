# ELO Automation Engine

ELO is a local automation engine that **observes events**, **asks Gemini for logic**, and **executes TypeScript automations**. It generates clean `.ts` scripts in `automations/` and runs them directly.

## ðŸŽ¯ Vision

ELO is meant to be a **proactive smartâ€‘home butler** that constantly adapts automations based on user preferences and daily device logs. The intended behavior is:

- Use AI decisions to **generate and refactor TypeScript automations**.
- Treat automations as living flows: **nothing is fixed**, and logic evolves as the userâ€™s tastes change.
- Learn from **continuous device logs** to keep the house optimized and responsive.

> Note: The current codebase does not implement this full vision yet; the section above describes the intended direction of the project.

## âœ… What it really does today

- **Creates automation scripts** in `automations/` (TypeScript).
- **Lists and updates automations** from the CLI.
- **Stores smartâ€‘home device logs** in `logs/events.jsonl` via the CLI.
- **Monitors registered devices** and logs only state changes.
- **Stores user requests** in `logs/requests.jsonl`.
- **Runs a decision loop** that updates automations from logs + requests + preferences.
- **Provides structured device context** (registry + status snapshot) to Gemini.
- **Learns preference patterns** from user decisions (accept/reject suggestions).
- **Optional network discovery** (mDNS) to detect new devices.

## ðŸš« What it does NOT do yet

- It **does not learn preferences automatically**; preferences come from recorded decisions and are summarized on demand.
- Preference learning today is **rule-based** (acceptance rate thresholds), not a full ML model.
- There is no sandboxing/permission system for automations yet.

## Project structure (real files in use)

- `src/cli/` â€” CLI entry and commands.
- `src/ai/` â€” Gemini CLI wrapper and prompt generator.
- `src/server/` â€” automation engine + device monitor + decision loop.
- `automations/` â€” TypeScript scripts generated by the CLI or AI.
- `logs/` â€” device logs stored as JSON Lines (created by the CLI).
- `logs/decisions.jsonl` â€” user acceptance/rejection history.
- `logs/requests.jsonl` â€” user requests history.
- `logs/devices.json` â€” device registry used by the monitor.

> Note: `logs/`, `automations/`, and `backups/` are local, user-specific, and not tracked in git.

## Setup

Install dependencies:

```bash
npm install
```

## Dockerized Engine (no local Gemini CLI needed)

The `elo-core` service runs the engine inside Docker, so your local machine does not need the Gemini CLI.
It builds from `docker/elo/Dockerfile` and includes **Google Cloud CLI + Gemini Code Assist (cloud-code-enterprise)**.
Running in production mode, it now compiles TypeScript (`npm run build`) and executes optimized JavaScript, reducing RAM usage.

Build arg supported:

- `GEMINI_CLI_INSTALL`: shell command to install your Gemini CLI inside the image (leave empty to skip).
- `GEMINI_GCLOUD_COMPONENT`: optional gcloud component to install (leave empty to skip).

The container uses these runtime environment variables (same as local usage):

- `GEMINI_CLI_BIN` (default: `gemini`)
- `GEMINI_CLI_ARGS` (extra CLI args)
- `GEMINI_CLI_PROMPT_ARG` (if your CLI uses a flag to pass the prompt)
- `GEMINI_API_KEY` (if set, uses Gemini API instead of CLI)
- `GEMINI_API_MODEL` (default: `gemini-1.5-flash`)
- `GEMINI_API_BASE_URL` (default: `https://generativelanguage.googleapis.com/v1beta`)
- `THINKING_BUDGET` (optional override; set to -1 for auto sizing)

### Gemini Code Assist CLI (Google Cloud)

The container image installs:

- `gcloud` (Google Cloud CLI)
- an optional Gemini Code Assist component (set `GEMINI_GCLOUD_COMPONENT`)

Authentication happens inside the container and is persisted in a volume (`gcloud_config`).
Run both logins once per environment:

```bash
docker compose run --rm --entrypoint gcloud elo-core auth login
docker compose run --rm --entrypoint gcloud elo-core auth application-default login
```

After authentication, set `GEMINI_CLI_BIN` + `GEMINI_CLI_ARGS` to the command that invokes Gemini Code Assist in your setup.
This project will call that command with the prompt you provide.

If you are unsure about the component name, run this inside the container to list available ones:

```bash
docker compose run --rm --entrypoint gcloud elo-core components list
```

## Gemini CLI + Google AI Studio

The CLI calls a local **Gemini CLI** binary. Configure your Gemini CLI separately (tokens, Google AI Studio, etc).

Environment variables used by the code:

- `GEMINI_CLI_BIN` (default: `gemini`)
- `GEMINI_CLI_ARGS` (extra CLI args)
- `GEMINI_CLI_PROMPT_ARG` (if your CLI uses a flag to pass the prompt)
- `GEMINI_API_KEY` (if set, uses Gemini API instead of CLI)
- `GEMINI_API_MODEL` (default: `gemini-1.5-flash`)
- `GEMINI_API_BASE_URL` (default: `https://generativelanguage.googleapis.com/v1beta`)
- `THINKING_BUDGET` (optional override; set to -1 for auto sizing)
- `ELO_FILES_PATH` (default: project root)
- `ELO_MONITOR_ENABLED` (default: true)
- `ELO_MONITOR_INTERVAL_MS` (default: 5000)
- `ELO_HEALTH_URL` (optional: endpoint to ping for health logging)
- `ELO_DISCOVERY_ENABLED` (default: true)
- `ELO_DISCOVERY_ACTIVE_SCAN` (default: true)
- `ELO_DISCOVERY_SUBNET` (optional: override subnet base, e.g. `192.168.1.0/24`)
- `ELO_DISCOVERY_RANGE` (optional: explicit range, e.g. `192.168.1.10-192.168.1.200`)
- `ELO_DISCOVERY_PORTS` (default: `4387,554,8899`)
- `ELO_DISCOVERY_SCAN_TIMEOUT_MS` (default: `250`)
- `ELO_DISCOVERY_SCAN_CONCURRENCY` (default: `64`)
- `ELO_DISCOVERY_SCAN_INTERVAL_MS` (default: `0` = only on startup)
- `ELO_GREE_BROADCAST_ENABLED` (default: true)
- `ELO_GREE_BROADCAST_PORTS` (default: `4387`)
- `ELO_GREE_BROADCAST_INTERVAL_MS` (default: `60000`)
- `ELO_GREE_BROADCAST_PAYLOAD` (default: `{"t":"scan"}`)
- `ELO_SSDP_ENABLED` (default: true)
- `ELO_SSDP_INTERVAL_MS` (default: `60000`)
- `ELO_FINGERPRINT_AI` (default: true when GEMINI_API_KEY is set)
- `ELO_FINGERPRINT_MODEL` (default: `gemini-2.5-flash`)
- `ELO_FINGERPRINT_TIMEOUT_MS` (default: `1500`)
- `ELO_WOL_ENABLED` (default: true)
- `ELO_WOL_BROADCAST` (default: `255.255.255.255`)
- `ELO_DECISION_LOOP_ENABLED` (default: true)
- `ELO_DECISION_INTERVAL_MS` (default: 10000)
- `ELO_DECISION_LOG_LIMIT` (default: 100)
- `ELO_DECISION_REQUEST_LIMIT` (default: 50)
- `ELO_DECISION_AUTOMATIONS` (comma-separated automation names)
- `ELO_AI_APPROVAL` (set to true to let Gemini decide approval thresholds)
- `ELO_AI_REPLY` (set to true to let Gemini parse ambiguous user replies)

If `GEMINI_API_KEY` is set, the CLI will call **Google AI Studio Gemini API** directly.
if it is not set, the CLI falls back to the local Gemini CLI binary.
Automation creation/refactor requests auto-scale the thinking budget between 4000â€“16000 based on context size.
Simple Q&A prompts use a thinking budget of 0.

Network discovery uses mDNS (including `_services._dns-sd._udp`, AirPlay, RAOP, printer) when the optional `bonjour-service` dependency is installed, plus an active TCP scan, SSDP listener, and a UDP broadcast for Gree devices. SSDP responses with Samsung/Tizen headers are tagged automatically. If `bonjour-service` is missing, ELO still performs the active scan/broadcast and logs the results.

### Optional: Nmap discovery script

If you want a deeper scan (similar to Home Assistant), you can run the bundled Nmap helper. It discovers live hosts and then scans selected TCP/UDP ports (including Samsung 8001/8002/1515), ingesting results into `logs/events.jsonl`. If a Samsung MAC is detected but those ports are closed, it will send a Wake-on-LAN packet.

The ingestion step also captures extra metadata (MAC/vendor, open ports, HTTP headers + title snippet, RTSP server banner, TCP banners, UDP probe responses, SSDP LOCATION parsing) to help the AI identify devices and protocols.

Environment knobs used by the script:

- `ELO_DISCOVERY_SUBNET` (optional, e.g. `192.168.16.0/24`)
- `ELO_NMAP_PORTS` (default: `80,443,8080,554,8899,8000,22`)
- `ELO_NMAP_UDP_PORTS` (default: `4387,1900`)

Run it from the project root:

```bash
chmod +x scripts/nmap-discovery.sh
sudo ./scripts/nmap-discovery.sh
```

## CLI usage (tested behavior)

```bash
npm run cli create-automation "Office Comfort" --ai --description "Keep office at 23C when occupied"
npm run cli list-automations
npm run cli update-automation "Office Comfort" --ai --log-limit 100
npm run cli add-log --device "thermostat" --event "temperature" --payload '{"value":23}'
npm run cli add-device --id "office-thermostat" --name "Thermostat" --room "office" --endpoint "http://localhost:8081/status"
npm run cli add-request --request "Ajuste o ar para 23C" --user "arthur" --context "office"
npm run cli record-decision --action-key "set-office-temp-23" --suggestion "Adjust office temp" --accepted
npm run cli summarize-preferences
npm run cli list-suggestions
npm run cli approve-suggestion <id>
npm run cli reject-suggestion <id>
npm run cli reply-suggestion <id> "Aham, mas coloca no 21c"
```

## Suggestion flow (proactivity)

When the decision loop proposes a change, it creates a **suggestion** entry (`logs/suggestions.jsonl`).

- If `ELO_AI_APPROVAL=true` or `GEMINI_API_KEY` is set, Gemini decides whether to auto-apply or request more approvals.
- Otherwise, a simple fallback rule is used (>=3 approvals, >=70% acceptance).

## Natural replies and lexicon

User replies are interpreted locally first, using `logs/lexicon.json` (optional). If the reply is long or ambiguous, ELO consults Gemini to extract intent and extra instructions.
When Gemini is used, it also returns the **matched term** (e.g., "claro", "nÃ£o"), which is stored back into `logs/lexicon.json` to adapt to the user's language.

## Example flow (smart-home butler)

1) **Ingest device logs** during the day:

```bash
npm run cli add-log --device "office-thermostat" --event "temperature" --payload '{"value":27}'
npm run cli add-log --device "office-thermostat" --event "temperature" --payload '{"value":28}'
```

2) **Record user decisions** when the butler suggests something:

```bash
npm run cli record-decision --action-key "set-office-temp-23" \
	--suggestion "Adjust office temperature to 23C and enable silent mode" \
	--accepted
```

3) **Review inferred preferences**:

```bash
npm run cli summarize-preferences
```

4) **Update the automation using AI + logs + preferences**:

```bash
npm run cli update-automation "Office Comfort" --ai --log-limit 100
```

## AI decision examples (based on device status + requests)

Example 1:

- User request: "Ligue o ar condicionado do escritÃ³rio"
- Device status: window is open
- Expected AI behavior: ask for confirmation before turning on AC.

Example 2:

- Calendar: meeting at 19h
- Device status: office at 29Â°C, window open
- Expected AI behavior: skip turning on AC until the window is closed, log the reason, and notify later.

## Docker (optional)

`docker-compose.yml` runs the ELO engine container with host networking for device discovery.
For active scans (Nmap/UDP), the container runs as root with `NET_RAW`/`NET_ADMIN` capabilities enabled.

```bash
docker-compose up --build
```

## Always-on monitoring

When the server starts, it polls registered devices (default 5s interval) and appends status logs only when state changes.

```bash
npm start
```

Environment variables:

- `ELO_MONITOR_ENABLED` (default: true)
- `ELO_MONITOR_INTERVAL_MS` (default: 5000) - Polling interval (5s) to save I/O
- `ELO_HEALTH_URL` (optional: health endpoint to ping)
- `ELO_DECISION_LOOP_ENABLED` (default: true)
- `ELO_DECISION_INTERVAL_MS` (default: 10000)
- `ELO_DECISION_LOG_LIMIT` (default: 100)
- `ELO_DECISION_REQUEST_LIMIT` (default: 50)
- `ELO_DECISION_AUTOMATIONS` (comma-separated automation names)

## Smoke test

This repo includes a small smoke test that validates file-based automation creation.

```bash
npm run smoke
```