# ELO Automation Engine

ELO is a local automation engine that **observes events**, **asks Gemini for logic**, and **executes TypeScript automations**. It no longer depends on n8n. Instead, it generates clean `.ts` scripts in `automations/` and runs them directly.

## ðŸŽ¯ Vision

ELO is meant to be a **proactive smartâ€‘home butler** that constantly adapts automations based on user preferences and daily device logs. The intended behavior is:

- Use AI decisions to **generate and refactor TypeScript automations**.
- Treat automations as living flows: **nothing is fixed**, and logic evolves as the userâ€™s tastes change.
- Learn from **continuous device logs** to keep the house optimized and responsive.

> Note: The current codebase does not implement this full vision yet; the section above describes the intended direction of the project.

## âœ… What it really does today

- **Creates automation scripts** in `automations/` (TypeScript).
- **Lists and updates automations** from the CLI.
- **Stores smartâ€‘home device logs** in `logs/events.jsonl` via the CLI.
- **Monitors registered devices** and logs only state changes.
- **Stores user requests** in `logs/requests.jsonl`.
- **Runs a decision loop** that updates automations from logs + requests + preferences.
- **Provides structured device context** (registry + status snapshot) to Gemini.
- **Learns preference patterns** from user decisions (accept/reject suggestions).
- **Optional network discovery** (mDNS) to detect new devices.

## ðŸš« What it does NOT do yet

- It **does not learn preferences automatically**; preferences come from recorded decisions and are summarized on demand.
- Preference learning today is **rule-based** (acceptance rate thresholds), not a full ML model.
- There is no sandboxing/permission system for automations yet.

## Project structure (real files in use)

- `src/cli/` â€” CLI entry and commands.
- `src/ai/` â€” Gemini CLI wrapper and prompt generator.
- `src/server/` â€” automation engine + device monitor + decision loop.
- `automations/` â€” TypeScript scripts generated by the CLI or AI.
- `logs/` â€” device logs stored as JSON Lines (created by the CLI).
- `logs/decisions.jsonl` â€” user acceptance/rejection history.
- `logs/requests.jsonl` â€” user requests history.
- `logs/devices.json` â€” device registry used by the monitor.

> Note: `logs/`, `automations/`, and `backups/` are local, user-specific, and not tracked in git.

## Setup

Install dependencies:

```bash
npm install
```

## Dockerized Engine (no local Gemini CLI needed)

The `elo-core` service runs the engine inside Docker, so your local machine does not need the Gemini CLI.
It builds from `docker/elo/Dockerfile` and includes **Google Cloud CLI + Gemini Code Assist (cloud-code-enterprise)**.
Running in production mode, it now compiles TypeScript (`npm run build`) and executes optimized JavaScript, reducing RAM usage.

Build arg supported:

- `GEMINI_CLI_INSTALL`: shell command to install your Gemini CLI inside the image (leave empty to skip).
- `GEMINI_GCLOUD_COMPONENT`: optional gcloud component to install (leave empty to skip).

The container uses these runtime environment variables (same as local usage):

- `GEMINI_CLI_BIN` (default: `gemini`)
- `GEMINI_CLI_ARGS` (extra CLI args)
- `GEMINI_CLI_PROMPT_ARG` (if your CLI uses a flag to pass the prompt)
- `GEMINI_API_KEY` (if set, uses Gemini API instead of CLI)
- `GEMINI_API_MODEL` (default: `gemini-1.5-flash`)
- `GEMINI_API_BASE_URL` (default: `https://generativelanguage.googleapis.com/v1beta`)
- `THINKING_BUDGET` (optional override; set to -1 for auto sizing)

### Gemini Code Assist CLI (Google Cloud)

The container image installs:

- `gcloud` (Google Cloud CLI)
- an optional Gemini Code Assist component (set `GEMINI_GCLOUD_COMPONENT`)

Authentication happens inside the container and is persisted in a volume (`gcloud_config`).
Run both logins once per environment:

```bash
docker compose run --rm --entrypoint gcloud elo-core auth login
docker compose run --rm --entrypoint gcloud elo-core auth application-default login
```

After authentication, set `GEMINI_CLI_BIN` + `GEMINI_CLI_ARGS` to the command that invokes Gemini Code Assist in your setup.
This project will call that command with the prompt you provide.

If you are unsure about the component name, run this inside the container to list available ones:

```bash
docker compose run --rm --entrypoint gcloud elo-core components list
```

## Gemini CLI + Google AI Studio

The CLI calls a local **Gemini CLI** binary. Configure your Gemini CLI separately (tokens, Google AI Studio, etc).

Environment variables used by the code:

- `GEMINI_CLI_BIN` (default: `gemini`)
- `GEMINI_CLI_ARGS` (extra CLI args)
- `GEMINI_CLI_PROMPT_ARG` (if your CLI uses a flag to pass the prompt)
- `GEMINI_API_KEY` (if set, uses Gemini API instead of CLI)
- `GEMINI_API_MODEL` (default: `gemini-1.5-flash`)
- `GEMINI_API_BASE_URL` (default: `https://generativelanguage.googleapis.com/v1beta`)
- `THINKING_BUDGET` (optional override; set to -1 for auto sizing)
- `ELO_FILES_PATH` (default: project root)
- `ELO_MONITOR_ENABLED` (default: true)
- `ELO_MONITOR_INTERVAL_MS` (default: 5000)
- `ELO_HEALTH_URL` (optional: endpoint to ping for health logging)
- `ELO_DISCOVERY_ENABLED` (default: true)
- `ELO_DECISION_LOOP_ENABLED` (default: true)
- `ELO_DECISION_INTERVAL_MS` (default: 10000)
- `ELO_DECISION_LOG_LIMIT` (default: 100)
- `ELO_DECISION_REQUEST_LIMIT` (default: 50)
- `ELO_DECISION_AUTOMATIONS` (comma-separated automation names)

If `GEMINI_API_KEY` is set, the CLI will call **Google AI Studio Gemini API** directly.
if it is not set, the CLI falls back to the local Gemini CLI binary.
Automation creation/refactor requests auto-scale the thinking budget between 4000â€“16000 based on context size.
Simple Q&A prompts use a thinking budget of 0.

Network discovery uses mDNS when the optional `bonjour-service` dependency is installed. If it is missing, ELO will log a warning and continue without discovery.

## CLI usage (tested behavior)

```bash
npm run cli create-automation "Office Comfort" --ai --description "Keep office at 23C when occupied"
npm run cli list-automations
npm run cli update-automation "Office Comfort" --ai --log-limit 100
npm run cli add-log --device "thermostat" --event "temperature" --payload '{"value":23}'
npm run cli add-device --id "office-thermostat" --name "Thermostat" --room "office" --endpoint "http://localhost:8081/status"
npm run cli add-request --request "Ajuste o ar para 23C" --user "arthur" --context "office"
npm run cli record-decision --action-key "set-office-temp-23" --suggestion "Adjust office temp" --accepted
npm run cli summarize-preferences
```

## Example flow (smart-home butler)

1) **Ingest device logs** during the day:

```bash
npm run cli add-log --device "office-thermostat" --event "temperature" --payload '{"value":27}'
npm run cli add-log --device "office-thermostat" --event "temperature" --payload '{"value":28}'
```

2) **Record user decisions** when the butler suggests something:

```bash
npm run cli record-decision --action-key "set-office-temp-23" \
	--suggestion "Adjust office temperature to 23C and enable silent mode" \
	--accepted
```

3) **Review inferred preferences**:

```bash
npm run cli summarize-preferences
```

4) **Update the automation using AI + logs + preferences**:

```bash
npm run cli update-automation "Office Comfort" --ai --log-limit 100
```

## AI decision examples (based on device status + requests)

Example 1:

- User request: "Ligue o ar condicionado do escritÃ³rio"
- Device status: window is open
- Expected AI behavior: ask for confirmation before turning on AC.

Example 2:

- Calendar: meeting at 19h
- Device status: office at 29Â°C, window open
- Expected AI behavior: skip turning on AC until the window is closed, log the reason, and notify later.

## Docker (optional)

`docker-compose.yml` runs the ELO engine container with host networking for device discovery.

```bash
docker-compose up --build
```

## Always-on monitoring

When the server starts, it polls registered devices (default 5s interval) and appends status logs only when state changes.

```bash
npm start
```

Environment variables:

- `ELO_MONITOR_ENABLED` (default: true)
- `ELO_MONITOR_INTERVAL_MS` (default: 5000) - Polling interval (5s) to save I/O
- `ELO_HEALTH_URL` (optional: health endpoint to ping)
- `ELO_DECISION_LOOP_ENABLED` (default: true)
- `ELO_DECISION_INTERVAL_MS` (default: 10000)
- `ELO_DECISION_LOG_LIMIT` (default: 100)
- `ELO_DECISION_REQUEST_LIMIT` (default: 50)
- `ELO_DECISION_AUTOMATIONS` (comma-separated automation names)

## Smoke test

This repo includes a small smoke test that validates file-based automation creation.

```bash
npm run smoke
```